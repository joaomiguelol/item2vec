{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we need\n",
    "train = train[['customer_id', 'article_id']]\n",
    "valid = valid[['customer_id', 'article_id']]\n",
    "# concat train and valid\n",
    "train = pd.concat([train, valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby customer_id and aggregate article_id into a list, then split the lists into groups of 2 consecutive articles\n",
    "train = train.groupby('customer_id')['article_id'].agg(list).apply(lambda x: [x[i:i+2] for i in range(len(x)-1)]).explode().reset_index()\n",
    "train = train[train['article_id'].notna()]\n",
    "# explode article_id into 2 columns article_1 and article_2\n",
    "train = train.join(pd.DataFrame(train.pop('article_id').tolist(), columns=['article_1', 'article_2']))\n",
    "# drop na values again\n",
    "train = train[train['article_1'].notna()]\n",
    "train = train[train['article_2'].notna()]\n",
    "# drop rows where article_1 and article_2 are the same\n",
    "train = train[train['article_1'] != train['article_2']]\n",
    "train['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_1</th>\n",
       "      <th>article_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>811927004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253154</th>\n",
       "      <td>1368904</td>\n",
       "      <td>884081001.0</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253155</th>\n",
       "      <td>1368904</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>762846027.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253156</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253157</th>\n",
       "      <td>1368904</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253158</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>882810001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          customer_id    article_1    article_2  label\n",
       "2                   1  599580055.0  811835004.0      1\n",
       "6                   1  811835004.0  723529001.0      1\n",
       "7                   1  723529001.0  559630026.0      1\n",
       "8                   1  559630026.0  599580083.0      1\n",
       "9                   1  599580083.0  811927004.0      1\n",
       "...               ...          ...          ...    ...\n",
       "11253154      1368904  884081001.0  794819001.0      1\n",
       "11253155      1368904  794819001.0  762846027.0      1\n",
       "11253156      1368904  866755002.0  840360003.0      1\n",
       "11253157      1368904  840360003.0  866755002.0      1\n",
       "11253158      1368904  866755002.0  882810001.0      1\n",
       "\n",
       "[9989748 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_2</th>\n",
       "      <th>article_1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869005001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>866111001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856667005.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>739461002.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679011009.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996995</th>\n",
       "      <td>662369058.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996996</th>\n",
       "      <td>841808001.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996997</th>\n",
       "      <td>687635018.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996998</th>\n",
       "      <td>805000002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996999</th>\n",
       "      <td>800447002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14997000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            article_2    article_1  label\n",
       "0         869005001.0  599580055.0      0\n",
       "1         866111001.0  599580055.0      0\n",
       "2         856667005.0  599580055.0      0\n",
       "3         739461002.0  599580055.0      0\n",
       "4         679011009.0  599580055.0      0\n",
       "...               ...          ...    ...\n",
       "14996995  662369058.0  795358001.0      0\n",
       "14996996  841808001.0  795358001.0      0\n",
       "14996997  687635018.0  795358001.0      0\n",
       "14996998  805000002.0  795358001.0      0\n",
       "14996999  800447002.0  795358001.0      0\n",
       "\n",
       "[14997000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to create negative samples, for each article_1 we need to randomly select N article_2 that is not the same as article_1\n",
    "# N is the number of negative samples we want\n",
    "N = 300\n",
    "\n",
    "# create a copy of train\n",
    "train_negative = train.copy()\n",
    "\n",
    "# create a list of all article_id\n",
    "article_ids = train['article_1'].unique()\n",
    "\n",
    "# for each article_id, randomly select N article_id that is not the same as article_id\n",
    "negative_samples = []\n",
    "for article_id in article_ids:\n",
    "    do_not_select = train[train['article_1'] == article_id]['article_2'].unique()\n",
    "    # randomly select N article_id that is not in do_not_select\n",
    "    negative_samples.extend(np.random.choice(np.setdiff1d(article_ids, do_not_select), N, replace=False))\n",
    "# create a dataframe from the negative samples\n",
    "train_negative = pd.DataFrame(negative_samples, columns=['article_2'])\n",
    "train_negative['article_1'] = article_ids.repeat(N)\n",
    "train_negative['label'] = 0\n",
    "\n",
    "train_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and train_negative    \n",
    "train = pd.concat([train, train_negative], ignore_index=True)\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 30% of train as valid\n",
    "valid = train.sample(frac=0.3, random_state=42)\n",
    "train = train.drop(valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'), index=False)\n",
    "valid.to_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(os.path.join(config.data_raw_dir , 'articles.parquet'))\n",
    "articles = articles[['article_id', 'prod_name', 'detail_desc']]\n",
    "\n",
    "# join prod_name and prod_desc\n",
    "articles['text'] = articles['prod_name'] + ' ' + articles['detail_desc']\n",
    "articles['text'] = articles['text'].str.lower()\n",
    "articles = articles[['article_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_articles(df, articles, article_column, text_column):\n",
    "    df = df.merge(articles, left_on=article_column, right_on='article_id', how='left')\n",
    "    df = df.rename(columns={'text': text_column})\n",
    "    return df.drop(columns=['article_id'])\n",
    "\n",
    "# Use the helper function to merge train and valid with article_1 and article_2\n",
    "train = merge_articles(train, articles, 'article_1', 'text_1')\n",
    "train = merge_articles(train, articles, 'article_2', 'text_2')\n",
    "\n",
    "valid = merge_articles(valid, articles, 'article_1', 'text_1')\n",
    "valid = merge_articles(valid, articles, 'article_2', 'text_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['customer_id'], inplace=True)\n",
    "valid.drop(columns=['customer_id'], inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "valid.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3590698311474374\n",
      "0.3590972014059759\n"
     ]
    }
   ],
   "source": [
    "print(valid.label.mean())\n",
    "print(train.label.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, valid], ignore_index=True)\n",
    "item_ids = set(df['article_1'].unique()).union(set(df['article_2'].unique()))\n",
    "# Create a dictionary that maps each item ID to a unique index\n",
    "vocab = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "num_items = len(set(df['article_1'].unique()).union(set(df['article_2'].unique())))\n",
    "del item_ids, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1749 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 128])\n",
      "torch.Size([4000, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([4000])) must be the same as input size (torch.Size([4000, 1, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(valid_data_loader), total_accuracy \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(valid_data_loader)))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m model \u001b[39m=\u001b[39m Item2Vec(num_items\u001b[39m=\u001b[39mnum_items, embedding_dim\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, input_tokens\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mtokenizer))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m train_model(model, data_loader, optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m), num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39m# Compute the dot product\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(output1, output2)  \u001b[39m# shape: (batch_size, 1, 1)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output, target\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_function\u001b[39m(output, target):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joao/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m nn\u001b[39m.\u001b[39;49mBCEWithLogitsLoss()(output, target)\n",
      "File \u001b[0;32m~/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    726\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    727\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    728\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/nn/functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3190\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3195\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([4000])) must be the same as input size (torch.Size([4000, 1, 1]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        # Create a dictionary that maps each item ID to a unique index\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', padding=True)\n",
    "        self.max_seq_length = 8\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # get article_id1 and article_id2 and labels\n",
    "        article_id1 = row['article_1']\n",
    "        article_id2 = row['article_2']\n",
    "        label = row['label']\n",
    "        # convert to torch tensors\n",
    "        article_id1 = torch.tensor(self.vocab[article_id1], dtype=torch.long)\n",
    "        article_id2 = torch.tensor(self.vocab[article_id2], dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        text_1 = self.tokenizer(\n",
    "            row['text_1'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        text_2 = self.tokenizer(\n",
    "            row['text_2'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # text_1 = torch.tensor(text_1['input_ids'][0], dtype=torch.long)\n",
    "        # text_2 = torch.tensor(text_2['input_ids'][0], dtype=torch.long)\n",
    "        text_1 = text_1['input_ids'][0]\n",
    "        text_2 = text_2['input_ids'][0]\n",
    "\n",
    "        return article_id1, article_id2, text_1, text_2, label\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "dataset = PairDataset(train, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=4000, shuffle=True)\n",
    "\n",
    "valid_dataset = PairDataset(valid, vocab)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=4000, shuffle=False)\n",
    "\n",
    "\n",
    "class Item2Vec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, input_tokens=30522):\n",
    "        super().__init__()\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        self.text_embeddings = nn.Embedding(input_tokens, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, 64, num_layers=1, batch_first=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.linear = nn.Linear(embedding_dim + 64, 512)\n",
    "        # activation function\n",
    "        self.act = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # output layer\n",
    "        self.output = nn.Linear(512, embedding_dim)\n",
    "        # output activation\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, item1, text):\n",
    "        embed = self.embeddings(item1)\n",
    "        # LSTM on text\n",
    "        text = self.text_embeddings(text)\n",
    "\n",
    "        lstm_out, _ = self.lstm(text)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        text = lstm_out[:, -1, :]\n",
    "\n",
    "        # Concatenate item embeddings and text embeddings\n",
    "        combined = torch.cat((embed, text), dim=1)\n",
    "\n",
    "        embed1 = self.dropout(combined)\n",
    "        # pass through dense layer\n",
    "        dense1 = self.linear(embed1)\n",
    "        # pass through activation function\n",
    "        act1 = self.act(dense1)\n",
    "        # pass through dropout\n",
    "        # pass through output layer\n",
    "        output = self.output(act1)\n",
    "        # pass through output activation\n",
    "        # output = self.output_act(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, optimizer, num_epochs):\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "            for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output1 = model(item1.to(device), text1.to(device)) #torch.Size([4000, 128])\n",
    "                output2 = model(item2.to(device), text2.to(device)) #torch.Size([4000, 128])\n",
    "                \n",
    "                print(output1.shape) #torch.Size([4000, 128])\n",
    "                print(output2.shape) #torch.Size([4000, 128])\n",
    "                dot_product = torch.sum(output1 * output2, dim=1)\n",
    "\n",
    "                # Apply sigmoid to convert the dot product to a similarity score\n",
    "                similarity_score = torch.sigmoid(dot_product)\n",
    "                # do the dot product between output1 and output2 to get the similarity score between the two items\n",
    "\n",
    "                \n",
    "                loss = loss_function(similarity_score, target.float().to(device))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "                progress_bar.update(1)\n",
    "                break\n",
    "            break\n",
    "                \n",
    "\n",
    "        # compute total loss and accuracy\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # switch model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "                \n",
    "                for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader): \n",
    "                    \n",
    "                    output1 = model(item1.to(device), text1.to(device))\n",
    "                    output2 = model(item2.to(device), text2.to(device))\n",
    "\n",
    "\n",
    "                    output1 = output1.view(output1.shape[0], 1, output1.shape[1])  # shape: (batch_size, 1, embedding_dim)\n",
    "                    output2 = output2.view(output2.shape[0], output2.shape[1], 1)  # shape: (batch_size, embedding_dim, 1)\n",
    "                    # Compute the dot product\n",
    "                    output = torch.bmm(output1, output2)  # shape: (batch_size, 1, 1)\n",
    "\n",
    "                    # Reshape the output to be two-dimensional\n",
    "                    output = output.view(output.shape[0], -1)  # shape: (batch_size, 1)\n",
    "                    target = target.view(target.shape[0], -1)  # shape: (batch_size, 1)\n",
    "\n",
    "                    loss = loss_function(output, target.float().to(device))\n",
    "\n",
    "                    # compute accuracy\n",
    "                    output = output.detach().cpu().numpy()\n",
    "                    target = target.detach().cpu().numpy()\n",
    "                    accuracy = ((output > 0) == target).mean()\n",
    "                    total_loss += loss.item()\n",
    "                    total_accuracy += accuracy\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "        print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, total_loss / len(valid_data_loader), total_accuracy / len(valid_data_loader)))\n",
    "\n",
    "\n",
    "model = Item2Vec(num_items=num_items, embedding_dim=128, input_tokens=len(dataset.tokenizer))\n",
    "train_model(model, data_loader, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
