{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we need\n",
    "train = train[['customer_id', 'article_id']]\n",
    "valid = valid[['customer_id', 'article_id']]\n",
    "# concat train and valid\n",
    "train = pd.concat([train, valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby customer_id and aggregate article_id into a list, then split the lists into groups of 2 consecutive articles\n",
    "train = train.groupby('customer_id')['article_id'].agg(list).apply(lambda x: [x[i:i+2] for i in range(len(x)-1)]).explode().reset_index()\n",
    "train = train[train['article_id'].notna()]\n",
    "# explode article_id into 2 columns article_1 and article_2\n",
    "train = train.join(pd.DataFrame(train.pop('article_id').tolist(), columns=['article_1', 'article_2']))\n",
    "# drop na values again\n",
    "train = train[train['article_1'].notna()]\n",
    "train = train[train['article_2'].notna()]\n",
    "# drop rows where article_1 and article_2 are the same\n",
    "train = train[train['article_1'] != train['article_2']]\n",
    "train['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_1</th>\n",
       "      <th>article_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>811927004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253154</th>\n",
       "      <td>1368904</td>\n",
       "      <td>884081001.0</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253155</th>\n",
       "      <td>1368904</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>762846027.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253156</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253157</th>\n",
       "      <td>1368904</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253158</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>882810001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          customer_id    article_1    article_2  label\n",
       "2                   1  599580055.0  811835004.0      1\n",
       "6                   1  811835004.0  723529001.0      1\n",
       "7                   1  723529001.0  559630026.0      1\n",
       "8                   1  559630026.0  599580083.0      1\n",
       "9                   1  599580083.0  811927004.0      1\n",
       "...               ...          ...          ...    ...\n",
       "11253154      1368904  884081001.0  794819001.0      1\n",
       "11253155      1368904  794819001.0  762846027.0      1\n",
       "11253156      1368904  866755002.0  840360003.0      1\n",
       "11253157      1368904  840360003.0  866755002.0      1\n",
       "11253158      1368904  866755002.0  882810001.0      1\n",
       "\n",
       "[9989748 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_2</th>\n",
       "      <th>article_1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869005001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>866111001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856667005.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>739461002.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679011009.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996995</th>\n",
       "      <td>662369058.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996996</th>\n",
       "      <td>841808001.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996997</th>\n",
       "      <td>687635018.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996998</th>\n",
       "      <td>805000002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996999</th>\n",
       "      <td>800447002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14997000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            article_2    article_1  label\n",
       "0         869005001.0  599580055.0      0\n",
       "1         866111001.0  599580055.0      0\n",
       "2         856667005.0  599580055.0      0\n",
       "3         739461002.0  599580055.0      0\n",
       "4         679011009.0  599580055.0      0\n",
       "...               ...          ...    ...\n",
       "14996995  662369058.0  795358001.0      0\n",
       "14996996  841808001.0  795358001.0      0\n",
       "14996997  687635018.0  795358001.0      0\n",
       "14996998  805000002.0  795358001.0      0\n",
       "14996999  800447002.0  795358001.0      0\n",
       "\n",
       "[14997000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to create negative samples, for each article_1 we need to randomly select N article_2 that is not the same as article_1\n",
    "# N is the number of negative samples we want\n",
    "N = 300\n",
    "\n",
    "# create a copy of train\n",
    "train_negative = train.copy()\n",
    "\n",
    "# create a list of all article_id\n",
    "article_ids = train['article_1'].unique()\n",
    "\n",
    "# for each article_id, randomly select N article_id that is not the same as article_id\n",
    "negative_samples = []\n",
    "for article_id in article_ids:\n",
    "    do_not_select = train[train['article_1'] == article_id]['article_2'].unique()\n",
    "    # randomly select N article_id that is not in do_not_select\n",
    "    negative_samples.extend(np.random.choice(np.setdiff1d(article_ids, do_not_select), N, replace=False))\n",
    "# create a dataframe from the negative samples\n",
    "train_negative = pd.DataFrame(negative_samples, columns=['article_2'])\n",
    "train_negative['article_1'] = article_ids.repeat(N)\n",
    "train_negative['label'] = 0\n",
    "\n",
    "train_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and train_negative    \n",
    "train = pd.concat([train, train_negative], ignore_index=True)\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 30% of train as valid\n",
    "valid = train.sample(frac=0.3, random_state=42)\n",
    "train = train.drop(valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'), index=False)\n",
    "valid.to_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(os.path.join(config.data_raw_dir , 'articles.parquet'))\n",
    "articles = articles[['article_id', 'prod_name', 'detail_desc']]\n",
    "\n",
    "# join prod_name and prod_desc\n",
    "articles['text'] = articles['prod_name'] + ' ' + articles['detail_desc']\n",
    "articles['text'] = articles['text'].str.lower()\n",
    "articles = articles[['article_id', 'text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "articles['text'] = articles['text'].apply(lambda x: tokenizer.encode(x, truncation=True, padding='max_length', max_length=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_articles(df, articles, article_column, text_column):\n",
    "    df = df.merge(articles, left_on=article_column, right_on='article_id', how='left')\n",
    "    df = df.rename(columns={'text': text_column})\n",
    "    return df.drop(columns=['article_id'])\n",
    "\n",
    "# Use the helper function to merge train and valid with article_1 and article_2\n",
    "train = merge_articles(train, articles, 'article_1', 'text_1')\n",
    "train = merge_articles(train, articles, 'article_2', 'text_2')\n",
    "\n",
    "valid = merge_articles(valid, articles, 'article_1', 'text_1')\n",
    "valid = merge_articles(valid, articles, 'article_2', 'text_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['customer_id'], inplace=True)\n",
    "valid.drop(columns=['customer_id'], inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "valid.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3590698311474374\n",
      "0.3590972014059759\n"
     ]
    }
   ],
   "source": [
    "print(valid.label.mean())\n",
    "print(train.label.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, valid], ignore_index=True)\n",
    "item_ids = set(df['article_1'].unique()).union(set(df['article_2'].unique()))\n",
    "# Create a dictionary that maps each item ID to a unique index\n",
    "vocab = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "num_items = len(set(df['article_1'].unique()).union(set(df['article_2'].unique())))\n",
    "del item_ids, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 126kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.53MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/1749 [00:44<2:41:30,  5.57s/it, loss=0.791]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(valid_data_loader), total_accuracy \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(valid_data_loader)))\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m model \u001b[39m=\u001b[39m Item2Vec(num_items\u001b[39m=\u001b[39mnum_items, embedding_dim\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, input_tokens\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mtokenizer))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m train_model(model, data_loader, optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m), num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(valid_data_loader)) \u001b[39mas\u001b[39;00m progress_bar:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (item1, item2, text1, text2, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(valid_data_loader):\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         output1 \u001b[39m=\u001b[39m model(item1\u001b[39m.\u001b[39mto(device), text1\u001b[39m.\u001b[39mto(device)) \u001b[39m#torch.Size([4000, 128])\u001b[39;00m\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m text_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     row[\u001b[39m'\u001b[39m\u001b[39mtext_1\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m text_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     row[\u001b[39m'\u001b[39;49m\u001b[39mtext_2\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length, \n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# text_1 = torch.tensor(text_1['input_ids'][0], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# text_2 = torch.tensor(text_2['input_ids'][0], dtype=torch.long)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/net/home/joao.miguel/Desktop/item2vec/notebooks/01-data-setup.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m text_1 \u001b[39m=\u001b[39m text_1[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2904\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2907\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2908\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2909\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2910\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2911\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2912\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2913\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2914\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2915\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2916\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2917\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2918\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2919\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2920\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2921\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2968\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2969\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2970\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m )\n\u001b[0;32m-> 2977\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2978\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2979\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2980\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2981\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2982\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2983\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2984\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2985\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2986\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2987\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2988\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2989\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2990\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2991\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2992\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2993\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2994\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2995\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2996\u001b[0m )\n",
      "File \u001b[0;32m/net/home/joao.miguel/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/transformers/tokenization_utils.py:663\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_token_to_id\u001b[39m(\u001b[39mself\u001b[39m, token):\n\u001b[1;32m    661\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    664\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    665\u001b[0m     text: Union[TextInput, PreTokenizedInput, EncodedInput],\n\u001b[1;32m    666\u001b[0m     text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    667\u001b[0m     add_special_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    668\u001b[0m     padding_strategy: PaddingStrategy \u001b[39m=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD,\n\u001b[1;32m    669\u001b[0m     truncation_strategy: TruncationStrategy \u001b[39m=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE,\n\u001b[1;32m    670\u001b[0m     max_length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    671\u001b[0m     stride: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[1;32m    672\u001b[0m     is_split_into_words: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m     pad_to_multiple_of: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m     return_tensors: Optional[Union[\u001b[39mstr\u001b[39m, TensorType]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m     return_token_type_ids: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m     return_attention_mask: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    677\u001b[0m     return_overflowing_tokens: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    678\u001b[0m     return_special_tokens_mask: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    679\u001b[0m     return_offsets_mapping: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    680\u001b[0m     return_length: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    681\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    682\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    683\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    684\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        # Create a dictionary that maps each item ID to a unique index\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', padding=True)\n",
    "        self.max_seq_length = 8\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # get article_id1 and article_id2 and labels\n",
    "        article_id1 = row['article_1']\n",
    "        article_id2 = row['article_2']\n",
    "        label = row['label']\n",
    "        # convert to torch tensors\n",
    "        article_id1 = torch.tensor(self.vocab[article_id1], dtype=torch.long)\n",
    "        article_id2 = torch.tensor(self.vocab[article_id2], dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        text_1 = self.tokenizer(\n",
    "            row['text_1'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        text_2 = self.tokenizer(\n",
    "            row['text_2'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # text_1 = torch.tensor(text_1['input_ids'][0], dtype=torch.long)\n",
    "        # text_2 = torch.tensor(text_2['input_ids'][0], dtype=torch.long)\n",
    "        text_1 = text_1['input_ids'][0]\n",
    "        text_2 = text_2['input_ids'][0]\n",
    "\n",
    "        return article_id1, article_id2, text_1, text_2, label\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "dataset = PairDataset(train, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "valid_dataset = PairDataset(valid, vocab)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=4000, shuffle=False)\n",
    "\n",
    "\n",
    "class Item2Vec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, input_tokens=30522):\n",
    "        super().__init__()\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        self.text_embeddings = nn.Embedding(input_tokens, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, 64, num_layers=1, batch_first=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.linear = nn.Linear(embedding_dim + 64, 512)\n",
    "        # activation function\n",
    "        self.act = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # output layer\n",
    "        self.output = nn.Linear(512, embedding_dim)\n",
    "        # output activation\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, item1, text):\n",
    "        embed = self.embeddings(item1)\n",
    "        # LSTM on text\n",
    "        text = self.text_embeddings(text)\n",
    "\n",
    "        lstm_out, _ = self.lstm(text)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        text = lstm_out[:, -1, :]\n",
    "\n",
    "        # Concatenate item embeddings and text embeddings\n",
    "        combined = torch.cat((embed, text), dim=1)\n",
    "\n",
    "        embed1 = self.dropout(combined)\n",
    "        # pass through dense layer\n",
    "        dense1 = self.linear(embed1)\n",
    "        # pass through activation function\n",
    "        act1 = self.act(dense1)\n",
    "        # pass through dropout\n",
    "        # pass through output layer\n",
    "        output = self.output(act1)\n",
    "        # pass through output activation\n",
    "        # output = self.output_act(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    return F.binary_cross_entropy(output, target.float(), reduction='mean')\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, optimizer, num_epochs):\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "            for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output1 = model(item1.to(device), text1.to(device)) #torch.Size([4000, 128])\n",
    "                output2 = model(item2.to(device), text2.to(device)) #torch.Size([4000, 128])\n",
    "                \n",
    "\n",
    "                dot_product = torch.sum(output1 * output2, dim=1)\n",
    "\n",
    "                # Apply sigmoid to convert the dot product to a similarity score\n",
    "                similarity_score = torch.sigmoid(dot_product)\n",
    "                \n",
    "                loss = loss_function(similarity_score, target.float().to(device))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                \n",
    "\n",
    "        # compute total loss and accuracy\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # switch model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "                \n",
    "                for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader): \n",
    "                    \n",
    "                    output1 = model(item1.to(device), text1.to(device))\n",
    "                    output2 = model(item2.to(device), text2.to(device))\n",
    "\n",
    "\n",
    "                    dot_product = torch.sum(output1 * output2, dim=1)\n",
    "\n",
    "                    # Apply sigmoid to convert the dot product to a similarity score\n",
    "                    similarity_score = torch.sigmoid(dot_product)\n",
    "                    \n",
    "                    loss = loss_function(similarity_score, target.float().to(device))\n",
    "\n",
    "\n",
    "                    # compute accuracy\n",
    "                    output = output.detach().cpu().numpy()\n",
    "                    target = target.detach().cpu().numpy()\n",
    "                    accuracy = ((output > 0) == target).mean()\n",
    "                    total_loss += loss.item()\n",
    "                    total_accuracy += accuracy\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "        print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, total_loss / len(valid_data_loader), total_accuracy / len(valid_data_loader)))\n",
    "\n",
    "\n",
    "model = Item2Vec(num_items=num_items, embedding_dim=128, input_tokens=len(dataset.tokenizer))\n",
    "train_model(model, data_loader, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
