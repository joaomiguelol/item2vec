{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we need\n",
    "train = train[['customer_id', 'article_id']]\n",
    "valid = valid[['customer_id', 'article_id']]\n",
    "# concat train and valid\n",
    "train = pd.concat([train, valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby customer_id and aggregate article_id into a list, then split the lists into groups of 2 consecutive articles\n",
    "train = train.groupby('customer_id')['article_id'].agg(list).apply(lambda x: [x[i:i+2] for i in range(len(x)-1)]).explode().reset_index()\n",
    "train = train[train['article_id'].notna()]\n",
    "# explode article_id into 2 columns article_1 and article_2\n",
    "train = train.join(pd.DataFrame(train.pop('article_id').tolist(), columns=['article_1', 'article_2']))\n",
    "# drop na values again\n",
    "train = train[train['article_1'].notna()]\n",
    "train = train[train['article_2'].notna()]\n",
    "# drop rows where article_1 and article_2 are the same\n",
    "train = train[train['article_1'] != train['article_2']]\n",
    "train['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_1</th>\n",
       "      <th>article_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>811835004.0</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>723529001.0</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>559630026.0</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>599580083.0</td>\n",
       "      <td>811927004.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253154</th>\n",
       "      <td>1368904</td>\n",
       "      <td>884081001.0</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253155</th>\n",
       "      <td>1368904</td>\n",
       "      <td>794819001.0</td>\n",
       "      <td>762846027.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253156</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253157</th>\n",
       "      <td>1368904</td>\n",
       "      <td>840360003.0</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253158</th>\n",
       "      <td>1368904</td>\n",
       "      <td>866755002.0</td>\n",
       "      <td>882810001.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          customer_id    article_1    article_2  label\n",
       "2                   1  599580055.0  811835004.0      1\n",
       "6                   1  811835004.0  723529001.0      1\n",
       "7                   1  723529001.0  559630026.0      1\n",
       "8                   1  559630026.0  599580083.0      1\n",
       "9                   1  599580083.0  811927004.0      1\n",
       "...               ...          ...          ...    ...\n",
       "11253154      1368904  884081001.0  794819001.0      1\n",
       "11253155      1368904  794819001.0  762846027.0      1\n",
       "11253156      1368904  866755002.0  840360003.0      1\n",
       "11253157      1368904  840360003.0  866755002.0      1\n",
       "11253158      1368904  866755002.0  882810001.0      1\n",
       "\n",
       "[9989748 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_2</th>\n",
       "      <th>article_1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>869005001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>866111001.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856667005.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>739461002.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679011009.0</td>\n",
       "      <td>599580055.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996995</th>\n",
       "      <td>662369058.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996996</th>\n",
       "      <td>841808001.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996997</th>\n",
       "      <td>687635018.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996998</th>\n",
       "      <td>805000002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996999</th>\n",
       "      <td>800447002.0</td>\n",
       "      <td>795358001.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14997000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            article_2    article_1  label\n",
       "0         869005001.0  599580055.0      0\n",
       "1         866111001.0  599580055.0      0\n",
       "2         856667005.0  599580055.0      0\n",
       "3         739461002.0  599580055.0      0\n",
       "4         679011009.0  599580055.0      0\n",
       "...               ...          ...    ...\n",
       "14996995  662369058.0  795358001.0      0\n",
       "14996996  841808001.0  795358001.0      0\n",
       "14996997  687635018.0  795358001.0      0\n",
       "14996998  805000002.0  795358001.0      0\n",
       "14996999  800447002.0  795358001.0      0\n",
       "\n",
       "[14997000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to create negative samples, for each article_1 we need to randomly select N article_2 that is not the same as article_1\n",
    "# N is the number of negative samples we want\n",
    "N = 300\n",
    "\n",
    "# create a copy of train\n",
    "train_negative = train.copy()\n",
    "\n",
    "# create a list of all article_id\n",
    "article_ids = train['article_1'].unique()\n",
    "\n",
    "# for each article_id, randomly select N article_id that is not the same as article_id\n",
    "negative_samples = []\n",
    "for article_id in article_ids:\n",
    "    do_not_select = train[train['article_1'] == article_id]['article_2'].unique()\n",
    "    # randomly select N article_id that is not in do_not_select\n",
    "    negative_samples.extend(np.random.choice(np.setdiff1d(article_ids, do_not_select), N, replace=False))\n",
    "# create a dataframe from the negative samples\n",
    "train_negative = pd.DataFrame(negative_samples, columns=['article_2'])\n",
    "train_negative['article_1'] = article_ids.repeat(N)\n",
    "train_negative['label'] = 0\n",
    "\n",
    "train_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and train_negative    \n",
    "train = pd.concat([train, train_negative], ignore_index=True)\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 30% of train as valid\n",
    "valid = train.sample(frac=0.3, random_state=42)\n",
    "train = train.drop(valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'), index=False)\n",
    "valid.to_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'), index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_processed_dir , 'train_pairs.parquet'))\n",
    "valid = pd.read_parquet(os.path.join(config.data_processed_dir , 'valid_pairs.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(os.path.join(config.data_raw_dir , 'articles.parquet'))\n",
    "articles = articles[['article_id', 'prod_name', 'detail_desc']]\n",
    "\n",
    "# join prod_name and prod_desc\n",
    "articles['text'] = articles['prod_name'] + ' ' + articles['detail_desc']\n",
    "articles['text'] = articles['text'].str.lower()\n",
    "articles = articles[['article_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_articles(df, articles, article_column, text_column):\n",
    "    df = df.merge(articles, left_on=article_column, right_on='article_id', how='left')\n",
    "    df = df.rename(columns={'text': text_column})\n",
    "    return df.drop(columns=['article_id'])\n",
    "\n",
    "# Use the helper function to merge train and valid with article_1 and article_2\n",
    "train = merge_articles(train, articles, 'article_1', 'text_1')\n",
    "train = merge_articles(train, articles, 'article_2', 'text_2')\n",
    "\n",
    "valid = merge_articles(valid, articles, 'article_1', 'text_1')\n",
    "valid = merge_articles(valid, articles, 'article_2', 'text_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['customer_id'], inplace=True)\n",
    "valid.drop(columns=['customer_id'], inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "valid.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3590698311474374\n",
      "0.3590972014059759\n"
     ]
    }
   ],
   "source": [
    "print(valid.label.mean())\n",
    "print(train.label.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, valid], ignore_index=True)\n",
    "item_ids = set(df['article_1'].unique()).union(set(df['article_2'].unique()))\n",
    "# Create a dictionary that maps each item ID to a unique index\n",
    "vocab = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "num_items = len(set(df['article_1'].unique()).union(set(df['article_2'].unique())))\n",
    "del item_ids, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Desktop/item2vec/.venv-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 591/1749 [22:26<44:16,  2.29s/it, loss=0.584]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        # Create a dictionary that maps each item ID to a unique index\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', padding=True)\n",
    "        self.max_seq_length = 8\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # get article_id1 and article_id2 and labels\n",
    "        article_id1 = row['article_1']\n",
    "        article_id2 = row['article_2']\n",
    "        label = row['label']\n",
    "        # convert to torch tensors\n",
    "        article_id1 = torch.tensor(self.vocab[article_id1], dtype=torch.long)\n",
    "        article_id2 = torch.tensor(self.vocab[article_id2], dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        text_1 = self.tokenizer(\n",
    "            row['text_1'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        text_2 = self.tokenizer(\n",
    "            row['text_2'], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # text_1 = torch.tensor(text_1['input_ids'][0], dtype=torch.long)\n",
    "        # text_2 = torch.tensor(text_2['input_ids'][0], dtype=torch.long)\n",
    "        text_1 = text_1['input_ids'][0]\n",
    "        text_2 = text_2['input_ids'][0]\n",
    "\n",
    "        return article_id1, article_id2, text_1, text_2, label\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "dataset = PairDataset(train, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "valid_dataset = PairDataset(valid, vocab)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=4000, shuffle=False)\n",
    "\n",
    "\n",
    "class Item2Vec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, input_tokens=30522):\n",
    "        super().__init__()\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        self.text_embeddings = nn.Embedding(input_tokens, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, 64, num_layers=1, batch_first=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.linear = nn.Linear(embedding_dim + 64, 512)\n",
    "        # activation function\n",
    "        self.act = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # output layer\n",
    "        self.output = nn.Linear(512, embedding_dim)\n",
    "        # output activation\n",
    "        self.output_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, item1, text):\n",
    "        embed = self.embeddings(item1)\n",
    "        # LSTM on text\n",
    "        text = self.text_embeddings(text)\n",
    "\n",
    "        lstm_out, _ = self.lstm(text)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        text = lstm_out[:, -1, :]\n",
    "\n",
    "        # Concatenate item embeddings and text embeddings\n",
    "        combined = torch.cat((embed, text), dim=1)\n",
    "\n",
    "        embed1 = self.dropout(combined)\n",
    "        # pass through dense layer\n",
    "        dense1 = self.linear(embed1)\n",
    "        # pass through activation function\n",
    "        act1 = self.act(dense1)\n",
    "        # pass through dropout\n",
    "        # pass through output layer\n",
    "        output = self.output(act1)\n",
    "        # pass through output activation\n",
    "        # output = self.output_act(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    return F.binary_cross_entropy(output, target.float(), reduction='mean')\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, optimizer, num_epochs):\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "            for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output1 = model(item1.to(device), text1.to(device)) #torch.Size([4000, 128])\n",
    "                output2 = model(item2.to(device), text2.to(device)) #torch.Size([4000, 128])\n",
    "                \n",
    "\n",
    "                dot_product = torch.sum(output1 * output2, dim=1)\n",
    "\n",
    "                # Apply sigmoid to convert the dot product to a similarity score\n",
    "                similarity_score = torch.sigmoid(dot_product)\n",
    "                \n",
    "                loss = loss_function(similarity_score, target.float().to(device))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "                progress_bar.update(1)\n",
    "\n",
    "                \n",
    "\n",
    "        # compute total loss and accuracy\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # switch model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=len(valid_data_loader)) as progress_bar:\n",
    "                \n",
    "                for i, (item1, item2, text1, text2, target) in enumerate(valid_data_loader): \n",
    "                    \n",
    "                    output1 = model(item1.to(device), text1.to(device))\n",
    "                    output2 = model(item2.to(device), text2.to(device))\n",
    "\n",
    "\n",
    "                    dot_product = torch.sum(output1 * output2, dim=1)\n",
    "\n",
    "                    # Apply sigmoid to convert the dot product to a similarity score\n",
    "                    similarity_score = torch.sigmoid(dot_product)\n",
    "                    \n",
    "                    loss = loss_function(similarity_score, target.float().to(device))\n",
    "\n",
    "\n",
    "                    # compute accuracy\n",
    "                    output = output.detach().cpu().numpy()\n",
    "                    target = target.detach().cpu().numpy()\n",
    "                    accuracy = ((output > 0) == target).mean()\n",
    "                    total_loss += loss.item()\n",
    "                    total_accuracy += accuracy\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "        print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, total_loss / len(valid_data_loader), total_accuracy / len(valid_data_loader)))\n",
    "\n",
    "\n",
    "model = Item2Vec(num_items=num_items, embedding_dim=128, input_tokens=len(dataset.tokenizer))\n",
    "train_model(model, data_loader, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
